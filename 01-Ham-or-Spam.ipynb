{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ham or Spam?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ The goal of this challenge is to classify emails as spams (1) or normal emails (0)\n",
    "\n",
    "üßπ First, you will apply cleaning techniques to these textual data\n",
    "\n",
    "üë©üèª‚Äçüî¨ Then, you will convert the cleaned texts into a numerical representation\n",
    "\n",
    "‚úâÔ∏è Eventually, you will apply the ***Multinomial Naive Bayes*** model to classify each email as either a spam or a regular email."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (0) The NTLK library (Natural Language Toolkit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T09:57:47.390744Z",
     "iopub.status.busy": "2025-05-16T09:57:47.390415Z",
     "iopub.status.idle": "2025-05-16T09:57:47.395583Z",
     "shell.execute_reply": "2025-05-16T09:57:47.394653Z",
     "shell.execute_reply.started": "2025-05-16T09:57:47.390722Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T09:57:47.650836Z",
     "iopub.status.busy": "2025-05-16T09:57:47.649845Z",
     "iopub.status.idle": "2025-05-16T09:57:50.997751Z",
     "shell.execute_reply": "2025-05-16T09:57:50.996227Z",
     "shell.execute_reply.started": "2025-05-16T09:57:47.650798Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/saranjthilak92/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/saranjthilak92/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/saranjthilak92/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/saranjthilak92/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/saranjthilak92/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When importing nltk for the first time, we need to also download a few built-in libraries\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')      # For nltk<3.9.0\n",
    "nltk.download('punkt_tab')  # For nltk>=3.9.0\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T09:57:51.000640Z",
     "iopub.status.busy": "2025-05-16T09:57:50.999877Z",
     "iopub.status.idle": "2025-05-16T09:57:52.035819Z",
     "shell.execute_reply": "2025-05-16T09:57:52.034351Z",
     "shell.execute_reply.started": "2025-05-16T09:57:51.000602Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: naturally irresistible your corporate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: the stock trading gunslinger  fanny i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: unbelievable new homes made easy  im ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: 4 color printing special  request add...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: do not have money , get software cds ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  spam\n",
       "0  Subject: naturally irresistible your corporate...     1\n",
       "1  Subject: the stock trading gunslinger  fanny i...     1\n",
       "2  Subject: unbelievable new homes made easy  im ...     1\n",
       "3  Subject: 4 color printing special  request add...     1\n",
       "4  Subject: do not have money , get software cds ...     1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"https://wagon-public-datasets.s3.amazonaws.com/05-Machine-Learning/10-Natural-Language-Processing/ham_spam_emails.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Cleaning the (text) dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is made up of emails that are classified as ham [0] or spam[1]. You need to clean the dataset before training a prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.1) Remove Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Create a function to remove the punctuation. Apply it to the `text` column and add the output to a new column in the dataframe called `clean_text` ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T09:57:52.037667Z",
     "iopub.status.busy": "2025-05-16T09:57:52.037178Z",
     "iopub.status.idle": "2025-05-16T09:57:52.356638Z",
     "shell.execute_reply": "2025-05-16T09:57:52.355115Z",
     "shell.execute_reply.started": "2025-05-16T09:57:52.037629Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  spam  \\\n",
      "0     Subject: naturally irresistible your corporate...     1   \n",
      "1     Subject: the stock trading gunslinger  fanny i...     1   \n",
      "2     Subject: unbelievable new homes made easy  im ...     1   \n",
      "3     Subject: 4 color printing special  request add...     1   \n",
      "4     Subject: do not have money , get software cds ...     1   \n",
      "...                                                 ...   ...   \n",
      "5723  Subject: re : research and development charges...     0   \n",
      "5724  Subject: re : receipts from visit  jim ,  than...     0   \n",
      "5725  Subject: re : enron case study update  wow ! a...     0   \n",
      "5726  Subject: re : interest  david ,  please , call...     0   \n",
      "5727  Subject: news : aurora 5 . 2 update  aurora ve...     0   \n",
      "\n",
      "                                             clean_text  \n",
      "0     Subject naturally irresistible your corporate ...  \n",
      "1     Subject the stock trading gunslinger  fanny is...  \n",
      "2     Subject unbelievable new homes made easy  im w...  \n",
      "3     Subject 4 color printing special  request addi...  \n",
      "4     Subject do not have money  get software cds fr...  \n",
      "...                                                 ...  \n",
      "5723  Subject re  research and development charges t...  \n",
      "5724  Subject re  receipts from visit  jim   thanks ...  \n",
      "5725  Subject re  enron case study update  wow  all ...  \n",
      "5726  Subject re  interest  david   please  call shi...  \n",
      "5727  Subject news  aurora 5  2 update  aurora versi...  \n",
      "\n",
      "[5728 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import string\n",
    "def remove_punctuation(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text\n",
    "# Apply function and create new column\n",
    "df['clean_text'] = df['text'].apply(remove_punctuation)\n",
    "\n",
    "# View result\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.2) Lower Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Create a function to lowercase the text. Apply it to `clean_text` ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T09:57:52.360118Z",
     "iopub.status.busy": "2025-05-16T09:57:52.359108Z",
     "iopub.status.idle": "2025-05-16T09:57:52.397964Z",
     "shell.execute_reply": "2025-05-16T09:57:52.395161Z",
     "shell.execute_reply.started": "2025-05-16T09:57:52.360087Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  spam  \\\n",
      "0     Subject: naturally irresistible your corporate...     1   \n",
      "1     Subject: the stock trading gunslinger  fanny i...     1   \n",
      "2     Subject: unbelievable new homes made easy  im ...     1   \n",
      "3     Subject: 4 color printing special  request add...     1   \n",
      "4     Subject: do not have money , get software cds ...     1   \n",
      "...                                                 ...   ...   \n",
      "5723  Subject: re : research and development charges...     0   \n",
      "5724  Subject: re : receipts from visit  jim ,  than...     0   \n",
      "5725  Subject: re : enron case study update  wow ! a...     0   \n",
      "5726  Subject: re : interest  david ,  please , call...     0   \n",
      "5727  Subject: news : aurora 5 . 2 update  aurora ve...     0   \n",
      "\n",
      "                                             clean_text  \n",
      "0     subject naturally irresistible your corporate ...  \n",
      "1     subject the stock trading gunslinger  fanny is...  \n",
      "2     subject unbelievable new homes made easy  im w...  \n",
      "3     subject 4 color printing special  request addi...  \n",
      "4     subject do not have money  get software cds fr...  \n",
      "...                                                 ...  \n",
      "5723  subject re  research and development charges t...  \n",
      "5724  subject re  receipts from visit  jim   thanks ...  \n",
      "5725  subject re  enron case study update  wow  all ...  \n",
      "5726  subject re  interest  david   please  call shi...  \n",
      "5727  subject news  aurora 5  2 update  aurora versi...  \n",
      "\n",
      "[5728 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Function to lowercase text\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Apply to the 'clean_text' column\n",
    "df['clean_text'] = df['clean_text'].apply(to_lowercase)\n",
    "\n",
    "# View result\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.3) Remove Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Create a function to remove numbers from the text. Apply it to `clean_text` ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T09:57:52.400460Z",
     "iopub.status.busy": "2025-05-16T09:57:52.399958Z",
     "iopub.status.idle": "2025-05-16T09:57:53.306798Z",
     "shell.execute_reply": "2025-05-16T09:57:53.305455Z",
     "shell.execute_reply.started": "2025-05-16T09:57:52.400412Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  spam  \\\n",
      "0     Subject: naturally irresistible your corporate...     1   \n",
      "1     Subject: the stock trading gunslinger  fanny i...     1   \n",
      "2     Subject: unbelievable new homes made easy  im ...     1   \n",
      "3     Subject: 4 color printing special  request add...     1   \n",
      "4     Subject: do not have money , get software cds ...     1   \n",
      "...                                                 ...   ...   \n",
      "5723  Subject: re : research and development charges...     0   \n",
      "5724  Subject: re : receipts from visit  jim ,  than...     0   \n",
      "5725  Subject: re : enron case study update  wow ! a...     0   \n",
      "5726  Subject: re : interest  david ,  please , call...     0   \n",
      "5727  Subject: news : aurora 5 . 2 update  aurora ve...     0   \n",
      "\n",
      "                                             clean_text  \n",
      "0     subject naturally irresistible your corporate ...  \n",
      "1     subject the stock trading gunslinger  fanny is...  \n",
      "2     subject unbelievable new homes made easy  im w...  \n",
      "3     subject  color printing special  request addit...  \n",
      "4     subject do not have money  get software cds fr...  \n",
      "...                                                 ...  \n",
      "5723  subject re  research and development charges t...  \n",
      "5724  subject re  receipts from visit  jim   thanks ...  \n",
      "5725  subject re  enron case study update  wow  all ...  \n",
      "5726  subject re  interest  david   please  call shi...  \n",
      "5727  subject news  aurora    update  aurora version...  \n",
      "\n",
      "[5728 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Function to remove numbers from text\n",
    "def remove_num(text):\n",
    "    return ''.join(char for char in text if not char.isdigit())\n",
    "\n",
    "# Apply to the 'clean_text' column\n",
    "df['clean_text'] = df['clean_text'].apply(remove_num)\n",
    "\n",
    "# View result\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.4) Remove StopWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Create a function to remove stopwords from the text. Apply it to `clean_text`. ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T10:05:29.364041Z",
     "iopub.status.busy": "2025-05-16T10:05:29.363754Z",
     "iopub.status.idle": "2025-05-16T10:05:34.056590Z",
     "shell.execute_reply": "2025-05-16T10:05:34.055146Z",
     "shell.execute_reply.started": "2025-05-16T10:05:29.364022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  spam  \\\n",
      "0     Subject: naturally irresistible your corporate...     1   \n",
      "1     Subject: the stock trading gunslinger  fanny i...     1   \n",
      "2     Subject: unbelievable new homes made easy  im ...     1   \n",
      "3     Subject: 4 color printing special  request add...     1   \n",
      "4     Subject: do not have money , get software cds ...     1   \n",
      "...                                                 ...   ...   \n",
      "5723  Subject: re : research and development charges...     0   \n",
      "5724  Subject: re : receipts from visit  jim ,  than...     0   \n",
      "5725  Subject: re : enron case study update  wow ! a...     0   \n",
      "5726  Subject: re : interest  david ,  please , call...     0   \n",
      "5727  Subject: news : aurora 5 . 2 update  aurora ve...     0   \n",
      "\n",
      "                                             clean_text  \n",
      "0     subject naturally irresistible corporate ident...  \n",
      "1     subject stock trading gunslinger fanny merrill...  \n",
      "2     subject unbelievable new homes made easy im wa...  \n",
      "3     subject color printing special request additio...  \n",
      "4     subject money get software cds software compat...  \n",
      "...                                                 ...  \n",
      "5723  subject research development charges gpg forwa...  \n",
      "5724  subject receipts visit jim thanks invitation v...  \n",
      "5725  subject enron case study update wow day super ...  \n",
      "5726  subject interest david please call shirley cre...  \n",
      "5727  subject news aurora update aurora version fast...  \n",
      "\n",
      "[5728 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove stop words\n",
    "def remove_stop_words(text):\n",
    "    tokens = word_tokenize(text)  # Tokenize inside the function\n",
    "    tokens_cleaned = [w for w in tokens if w.lower() not in stop_words]  \n",
    "\n",
    "# Apply to 'clean_text'\n",
    "df['clean_text'] = df['clean_text'].apply(remove_stop_words)\n",
    "\n",
    "# View result\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.5) Lemmatize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Create a function to lemmatize the text. Make sure the output is a single string, not a list of words. Apply it to `clean_text`. ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T10:21:57.653302Z",
     "iopub.status.busy": "2025-05-16T10:21:57.652895Z",
     "iopub.status.idle": "2025-05-16T10:22:13.037263Z",
     "shell.execute_reply": "2025-05-16T10:22:13.035919Z",
     "shell.execute_reply.started": "2025-05-16T10:21:57.653277Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  spam  \\\n",
      "0     Subject: naturally irresistible your corporate...     1   \n",
      "1     Subject: the stock trading gunslinger  fanny i...     1   \n",
      "2     Subject: unbelievable new homes made easy  im ...     1   \n",
      "3     Subject: 4 color printing special  request add...     1   \n",
      "4     Subject: do not have money , get software cds ...     1   \n",
      "...                                                 ...   ...   \n",
      "5723  Subject: re : research and development charges...     0   \n",
      "5724  Subject: re : receipts from visit  jim ,  than...     0   \n",
      "5725  Subject: re : enron case study update  wow ! a...     0   \n",
      "5726  Subject: re : interest  david ,  please , call...     0   \n",
      "5727  Subject: news : aurora 5 . 2 update  aurora ve...     0   \n",
      "\n",
      "                                             clean_text  \n",
      "0     subject naturally irresistible corporate ident...  \n",
      "1     subject stock trade gunslinger fanny merrill m...  \n",
      "2     subject unbelievable new home make easy im wan...  \n",
      "3     subject color print special request additional...  \n",
      "4     subject money get software cd software compati...  \n",
      "...                                                 ...  \n",
      "5723  subject research development charge gpg forwar...  \n",
      "5724  subject receipt visit jim thank invitation vis...  \n",
      "5725  subject enron case study update wow day super ...  \n",
      "5726  subject interest david please call shirley cre...  \n",
      "5727  subject news aurora update aurora version fast...  \n",
      "\n",
      "[5728 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Function to lemmatize\n",
    "def lemmatize(text):\n",
    "    tokens = word_tokenize(text)  # Tokenize inside the function\n",
    "    tokens_cleaned = [w for w in tokens if w.lower() not in stop_words]\n",
    "    # Lemmatizing the verbs\n",
    "    verb_lemmatized = [\n",
    "    WordNetLemmatizer().lemmatize(word, pos = \"v\") # v --> verbs\n",
    "    for word in tokens_cleaned\n",
    "    ]\n",
    "\n",
    "    # 2 - Lemmatizing the nouns\n",
    "    noun_lemmatized = [\n",
    "    WordNetLemmatizer().lemmatize(word, pos = \"n\") # n --> nouns\n",
    "    for word in verb_lemmatized\n",
    "    ]\n",
    "    return ' '.join(noun_lemmatized)\n",
    "# Apply to 'clean_text'\n",
    "df['clean_text'] = df['clean_text'].apply(lemmatize)\n",
    "\n",
    "# View result\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Bag-of-words Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.1) Digitizing the textual data into numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Vectorize the `clean_text` to a Bag-of-Words representation with a default CountVectorizer. Save as `X_bow`. ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T11:00:54.956378Z",
     "iopub.status.busy": "2025-05-16T11:00:54.955945Z",
     "iopub.status.idle": "2025-05-16T11:00:56.305707Z",
     "shell.execute_reply": "2025-05-16T11:00:56.304622Z",
     "shell.execute_reply.started": "2025-05-16T11:00:54.956357Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5728, 28173)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaaenerfax</th>\n",
       "      <th>aadedeji</th>\n",
       "      <th>aagrawal</th>\n",
       "      <th>aal</th>\n",
       "      <th>aaldous</th>\n",
       "      <th>aaliyah</th>\n",
       "      <th>aall</th>\n",
       "      <th>aanalysis</th>\n",
       "      <th>...</th>\n",
       "      <th>zwzm</th>\n",
       "      <th>zxghlajf</th>\n",
       "      <th>zyban</th>\n",
       "      <th>zyc</th>\n",
       "      <th>zygoma</th>\n",
       "      <th>zymg</th>\n",
       "      <th>zzmacmac</th>\n",
       "      <th>zzn</th>\n",
       "      <th>zzncacst</th>\n",
       "      <th>zzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5723</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5724</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5725</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5726</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5727</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5728 rows √ó 28173 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aa  aaa  aaaenerfax  aadedeji  aagrawal  aal  aaldous  aaliyah  aall  \\\n",
       "0      0    0           0         0         0    0        0        0     0   \n",
       "1      0    0           0         0         0    0        0        0     0   \n",
       "2      0    0           0         0         0    0        0        0     0   \n",
       "3      0    0           0         0         0    0        0        0     0   \n",
       "4      0    0           0         0         0    0        0        0     0   \n",
       "...   ..  ...         ...       ...       ...  ...      ...      ...   ...   \n",
       "5723   0    0           0         0         0    0        0        0     0   \n",
       "5724   0    0           0         0         0    0        0        0     0   \n",
       "5725   0    0           0         0         0    0        0        0     0   \n",
       "5726   0    0           0         0         0    0        0        0     0   \n",
       "5727   0    0           0         0         0    0        0        0     0   \n",
       "\n",
       "      aanalysis  ...  zwzm  zxghlajf  zyban  zyc  zygoma  zymg  zzmacmac  zzn  \\\n",
       "0             0  ...     0         0      0    0       0     0         0    0   \n",
       "1             0  ...     0         0      0    0       0     0         0    0   \n",
       "2             0  ...     0         0      0    0       0     0         0    0   \n",
       "3             0  ...     0         0      0    0       0     0         0    0   \n",
       "4             0  ...     0         0      0    0       0     0         0    0   \n",
       "...         ...  ...   ...       ...    ...  ...     ...   ...       ...  ...   \n",
       "5723          0  ...     0         0      0    0       0     0         0    0   \n",
       "5724          0  ...     0         0      0    0       0     0         0    0   \n",
       "5725          0  ...     0         0      0    0       0     0         0    0   \n",
       "5726          0  ...     0         0      0    0       0     0         0    0   \n",
       "5727          0  ...     0         0      0    0       0     0         0    0   \n",
       "\n",
       "      zzncacst  zzzz  \n",
       "0            0     0  \n",
       "1            0     0  \n",
       "2            0     0  \n",
       "3            0     0  \n",
       "4            0     0  \n",
       "...        ...   ...  \n",
       "5723         0     0  \n",
       "5724         0     0  \n",
       "5725         0     0  \n",
       "5726         0     0  \n",
       "5727         0     0  \n",
       "\n",
       "[5728 rows x 28173 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 1. Initialize the vectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# 2. Fit and transform the cleaned text\n",
    "X_bow = count_vectorizer.fit_transform(df['clean_text'])\n",
    "\n",
    "# 3. View the result\n",
    "print(X_bow.shape)\n",
    "\n",
    "# Optionally, convert to array or DataFrame to inspect\n",
    "X_bow_array = X_bow.toarray()\n",
    "pd.DataFrame(X_bow_array, columns=vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.2) Multinomial Naive Bayes Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Cross-validate a MultinomialNB model with the bag-of-words data. Score the model's accuracy. ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T11:00:56.307281Z",
     "iopub.status.busy": "2025-05-16T11:00:56.307004Z",
     "iopub.status.idle": "2025-05-16T11:00:56.391620Z",
     "shell.execute_reply": "2025-05-16T11:00:56.390384Z",
     "shell.execute_reply.started": "2025-05-16T11:00:56.307258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated accuracy scores: [0.9877836  0.98516579 0.9921466  0.98515284 0.99213974]\n",
      "Mean accuracy: 0.99\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = MultinomialNB()\n",
    "accuracy_scores = cross_val_score(clf, X_bow, y, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"Cross-validated accuracy scores:\", accuracy_scores)\n",
    "print(f\"Mean accuracy: {accuracy_scores.mean():.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ Congratulations !\n",
    "\n",
    "üíæ Don't forget to git add/commit/push your notebook...\n",
    "\n",
    "üöÄ ... and move on to the next challenge !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
